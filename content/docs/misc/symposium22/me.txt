The quantification of uncertainty is an important topic when it comes to modeling function landscapes based on previously evaluated input-output pairs. Gaussian process regression and the closely related Kriging method form allegedly the most well known class of surrogate models supporting uncertainty quantification. Here the uncertainty stems from the assumption that outputs are correlated by means of a distance (in the input-space) dependent correlation function. Such knowledge can be used to compute probabilistic confidence bounds to quantify uncertainty of predictions. Lipschitz continuity (and the more general HÃ¶lder continuity) on the other hand makes assumptions about bounded change rates. It, too, is based on distances in the input space but its model assumptions yield deterministic upper and lower bounds for the uncertainty ranges in prediction tasks.<br>
We contrast these two techniques and reveal commonalities and differences and also comment on their usefulness in the integration to (multiobjective) Bayesian optimization frameworks. A special focus will be on variants of expected improvements and show that the use of a Lipschitzian interpretation of the Expected Improvement is almost equivalent to Shubert's algorithm  Computations in the Lipschitzian case are far easier and more efficient while many of the interesting properties of Gaussian process models are preserved. 
